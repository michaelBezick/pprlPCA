Job 9541096 / array task  on gilbreth-f003.rcac.purdue.edu
CUDA_VISIBLE_DEVICES=0
Group name is:  DR_TF
Instantiating 8 environments...
Environments instantiated.
Instantiating 1 environments...
Environments instantiated.
Instantiating 1 environments...
Environments instantiated.
Instantiating 1 environments...
Environments instantiated.
Instantiating 1 environments...
Environments instantiated.
Instantiating 1 environments...
Environments instantiated.
Instantiating 1 environments...
Environments instantiated.
Instantiating 1 environments...
Environments instantiated.
Instantiating 1 environments...
Environments instantiated.
Instantiating 1 environments...
Environments instantiated.
BasicSampler: Resetting all environments.
BasicSampler: Resetting agent.
AuxPcSAC: Given sampler batch size 5120, training batch size 128, and replay ratio 64, there will be 2560 updates per iteration.
AuxPcSAC: Using learnable entropy coefficient with target entropy of -8
RLRunner: Starting training...
RLRunner: Saving log files to /scratch/gilbreth/mbezick/pprlPCA2/wandb/run-20250911_141751-oe7u5wnk/files
RLRunner: Evaluating agent...
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
Saved video of policy to videos/2025-09-11/oe7u5wnk/nominal/policy_step_0.mp4.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
RLRunner: Finished evaluating agent.
----------------------------------
| eval/               |          |
|    Length           | 200      |
|    Return           | 0.616    |
|    NonzeroRewards   | 200      |
|    DiscountedReturn | 0.616    |
|    Success          | 0        |
|    SuccessLength    | 200      |
| time/               |          |
|    iterations       | 0        |
|    fps              | 0        |
|    elapsed_time     | 123      |
|    elapsed_steps    | 0        |
----------------------------------
For the first 10000 steps, agent will use a fixed std of 0.75 for exploration.
----------------------------------
| rollout/            |          |
|    Length           | 200      |
|    Return           | 0.579    |
|    NonzeroRewards   | 200      |
|    DiscountedReturn | 0.579    |
|    Success          | 0        |
|    SuccessLength    | 200      |
| time/               |          |
|    iterations       | 1        |
|    fps              | 131      |
|    elapsed_time     | 162      |
|    elapsed_steps    | 5120     |
----------------------------------
-----------------------------------
| rollout/             |          |
|    Length            | 200      |
|    Return            | 0.504    |
|    NonzeroRewards    | 200      |
|    DiscountedReturn  | 0.504    |
|    Success           | 0        |
|    SuccessLength     | 200      |
| algo/                |          |
|    critic_loss       | 0.00125  |
|    mean_entropy      | 5.46     |
|    mean_ent_bonus    | 0.482    |
|    max_target_q      | 2.15     |
|    min_target_q      | 2.1      |
|    max_reward        | 0.0107   |
|    min_reward        | 0.00113  |
|    chamfer_loss      | 0.000738 |
|    encoder_grad_norm | 0.0307   |
|    q1_grad_norm      | 0.109    |
|    q2_grad_norm      | 0.104    |
|    actor_loss        | -2.7     |
|    ent_coeff         | 0.0882   |
|    ent_coeff_loss    | -32.7    |
|    pi_grad_norm      | 0.0912   |
|    n_updates         | 2560     |
| time/                |          |
|    iterations        | 2        |
|    fps               | 6.1      |
|    elapsed_time      | 1e+03    |
|    elapsed_steps     | 10240    |
-----------------------------------
RLRunner: Evaluating agent...
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
Saved video of policy to videos/2025-09-11/oe7u5wnk/nominal/policy_step_15360.mp4.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
RLRunner: Finished evaluating agent.
-----------------------------------
| rollout/             |          |
|    Length            | 200      |
|    Return            | 0.677    |
|    NonzeroRewards    | 200      |
|    DiscountedReturn  | 0.677    |
|    Success           | 0        |
|    SuccessLength     | 200      |
| algo/                |          |
|    critic_loss       | 0.000527 |
|    mean_entropy      | 5.46     |
|    mean_ent_bonus    | 0.373    |
|    max_target_q      | 2.29     |
|    min_target_q      | 2.24     |
|    max_reward        | 0.0141   |
|    min_reward        | 0.00113  |
|    chamfer_loss      | 0.000389 |
|    encoder_grad_norm | 0.0117   |
|    q1_grad_norm      | 0.0645   |
|    q2_grad_norm      | 0.0633   |
|    actor_loss        | -2.61    |
|    ent_coeff         | 0.0683   |
|    ent_coeff_loss    | -36.2    |
|    pi_grad_norm      | 0.0506   |
|    n_updates         | 5120     |
| eval/                |          |
|    Length            | 200      |
|    Return            | 0.488    |
|    NonzeroRewards    | 200      |
|    DiscountedReturn  | 0.488    |
|    Success           | 0        |
|    SuccessLength     | 200      |
| time/                |          |
|    iterations        | 3        |
|    fps               | 5.37     |
|    elapsed_time      | 1.95e+03 |
|    elapsed_steps     | 15360    |
-----------------------------------
-----------------------------------
| rollout/             |          |
|    Length            | 200      |
|    Return            | 0.582    |
|    NonzeroRewards    | 200      |
|    DiscountedReturn  | 0.582    |
|    Success           | 0        |
|    SuccessLength     | 200      |
| algo/                |          |
|    critic_loss       | 0.000308 |
|    mean_entropy      | 5.46     |
|    mean_ent_bonus    | 0.289    |
|    max_target_q      | 1.8      |
|    min_target_q      | 1.74     |
|    max_reward        | 0.0135   |
|    min_reward        | 0.00113  |
|    chamfer_loss      | 0.000306 |
|    encoder_grad_norm | 0.00902  |
|    q1_grad_norm      | 0.0398   |
|    q2_grad_norm      | 0.0391   |
|    actor_loss        | -2.04    |
|    ent_coeff         | 0.0529   |
|    ent_coeff_loss    | -39.6    |
|    pi_grad_norm      | 0.0358   |
|    n_updates         | 7680     |
| time/                |          |
|    iterations        | 4        |
|    fps               | 6.25     |
|    elapsed_time      | 2.77e+03 |
|    elapsed_steps     | 20480    |
-----------------------------------
-----------------------------------
| rollout/             |          |
|    Length            | 200      |
|    Return            | 0.573    |
|    NonzeroRewards    | 200      |
|    DiscountedReturn  | 0.573    |
|    Success           | 0        |
|    SuccessLength     | 200      |
| algo/                |          |
|    critic_loss       | 0.000178 |
|    mean_entropy      | 5.47     |
|    mean_ent_bonus    | 0.224    |
|    max_target_q      | 1.41     |
|    min_target_q      | 1.35     |
|    max_reward        | 0.0128   |
|    min_reward        | 0.00113  |
|    chamfer_loss      | 0.000259 |
|    encoder_grad_norm | 0.00784  |
|    q1_grad_norm      | 0.0239   |
|    q2_grad_norm      | 0.0227   |
|    actor_loss        | -1.59    |
|    ent_coeff         | 0.0409   |
|    ent_coeff_loss    | -43.1    |
|    pi_grad_norm      | 0.0252   |
|    n_updates         | 10240    |
| time/                |          |
|    iterations        | 5        |
|    fps               | 6.23     |
|    elapsed_time      | 3.6e+03  |
|    elapsed_steps     | 25600    |
-----------------------------------
RLRunner: Evaluating agent...
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
Saved video of policy to videos/2025-09-11/oe7u5wnk/nominal/policy_step_30720.mp4.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
RLRunner: Finished evaluating agent.
-----------------------------------
| rollout/             |          |
|    Length            | 200      |
|    Return            | 0.682    |
|    NonzeroRewards    | 200      |
|    DiscountedReturn  | 0.682    |
|    Success           | 0        |
|    SuccessLength     | 200      |
| algo/                |          |
|    critic_loss       | 0.000108 |
|    mean_entropy      | 5.47     |
|    mean_ent_bonus    | 0.173    |
|    max_target_q      | 1.11     |
|    min_target_q      | 1.05     |
|    max_reward        | 0.0139   |
|    min_reward        | 0.00114  |
|    chamfer_loss      | 0.000237 |
|    encoder_grad_norm | 0.00724  |
|    q1_grad_norm      | 0.0167   |
|    q2_grad_norm      | 0.0157   |
|    actor_loss        | -1.23    |
|    ent_coeff         | 0.0317   |
|    ent_coeff_loss    | -46.5    |
|    pi_grad_norm      | 0.0186   |
|    n_updates         | 12800    |
| eval/                |          |
|    Length            | 200      |
|    Return            | 0.704    |
|    NonzeroRewards    | 200      |
|    DiscountedReturn  | 0.704    |
|    Success           | 0        |
|    SuccessLength     | 200      |
| time/                |          |
|    iterations        | 6        |
|    fps               | 5.37     |
|    elapsed_time      | 4.55e+03 |
|    elapsed_steps     | 30720    |
-----------------------------------
-----------------------------------
| rollout/             |          |
|    Length            | 200      |
|    Return            | 0.69     |
|    NonzeroRewards    | 200      |
|    DiscountedReturn  | 0.69     |
|    Success           | 0        |
|    SuccessLength     | 200      |
| algo/                |          |
|    critic_loss       | 0.000367 |
|    mean_entropy      | 5.47     |
|    mean_ent_bonus    | 0.134    |
|    max_target_q      | 0.847    |
|    min_target_q      | 0.791    |
|    max_reward        | 0.0134   |
|    min_reward        | 0.00114  |
|    chamfer_loss      | 0.000231 |
|    encoder_grad_norm | 0.00747  |
|    q1_grad_norm      | 0.0112   |
|    q2_grad_norm      | 0.0148   |
|    actor_loss        | -0.945   |
|    ent_coeff         | 0.0245   |
|    ent_coeff_loss    | -50      |
|    pi_grad_norm      | 0.0139   |
|    n_updates         | 15360    |
| time/                |          |
|    iterations        | 7        |
|    fps               | 6.12     |
|    elapsed_time      | 5.38e+03 |
|    elapsed_steps     | 35840    |
-----------------------------------
-----------------------------------
| rollout/             |          |
|    Length            | 200      |
|    Return            | 0.834    |
|    NonzeroRewards    | 200      |
|    DiscountedReturn  | 0.834    |
|    Success           | 0        |
|    SuccessLength     | 200      |
| algo/                |          |
|    critic_loss       | 3.49e-05 |
|    mean_entropy      | 5.47     |
|    mean_ent_bonus    | 0.104    |
|    max_target_q      | 0.684    |
|    min_target_q      | 0.623    |
|    max_reward        | 0.0132   |
|    min_reward        | 0.00114  |
|    chamfer_loss      | 0.000212 |
|    encoder_grad_norm | 0.00643  |
|    q1_grad_norm      | 0.00554  |
|    q2_grad_norm      | 0.00399  |
|    actor_loss        | -0.747   |
|    ent_coeff         | 0.019    |
|    ent_coeff_loss    | -53.4    |
|    pi_grad_norm      | 0.0106   |
|    n_updates         | 17920    |
| time/                |          |
|    iterations        | 8        |
|    fps               | 6.14     |
|    elapsed_time      | 6.22e+03 |
|    elapsed_steps     | 40960    |
-----------------------------------
RLRunner: Evaluating agent...
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
Saved video of policy to videos/2025-09-11/oe7u5wnk/nominal/policy_step_46080.mp4.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
EvalSampler: Resetting all environments.
EvalSampler: Resetting agent.
RLRunner: Finished evaluating agent.
-----------------------------------
| rollout/             |          |
|    Length            | 200      |
|    Return            | 0.772    |
|    NonzeroRewards    | 200      |
|    DiscountedReturn  | 0.772    |
|    Success           | 0        |
|    SuccessLength     | 200      |
| algo/                |          |
|    critic_loss       | 2.53e-05 |
|    mean_entropy      | 5.46     |
|    mean_ent_bonus    | 0.0803   |
|    max_target_q      | 0.56     |
|    min_target_q      | 0.485    |
|    max_reward        | 0.014    |
|    min_reward        | 0.00114  |
|    chamfer_loss      | 0.000204 |
|    encoder_grad_norm | 0.00624  |
|    q1_grad_norm      | 0.00485  |
|    q2_grad_norm      | 0.00511  |
|    actor_loss        | -0.584   |
|    ent_coeff         | 0.0147   |
|    ent_coeff_loss    | -56.9    |
|    pi_grad_norm      | 0.00814  |
|    n_updates         | 20480    |
| eval/                |          |
|    Length            | 200      |
|    Return            | 0.77     |
|    NonzeroRewards    | 200      |
|    DiscountedReturn  | 0.77     |
|    Success           | 0        |
|    SuccessLength     | 200      |
| time/                |          |
|    iterations        | 9        |
|    fps               | 5.35     |
|    elapsed_time      | 7.18e+03 |
|    elapsed_steps     | 46080    |
-----------------------------------
